input 
{
	file  # read from file 
	{
		path => "D:/LSQ-latest-updated-Docuement/LSQ-latest-updated-Docuement/Docker-Compose-Elastic/logstash_pipeline/csv/500-greatest-hists.csv"
		start_position => "beginning"
		sincedb_path => "/dev/null"
		add_field => 
		{ 
			"source" => "500GreatestHits" 
		}
	}
}
filter 
{
	if [source] == "500GreatestHits"
	{
		csv  # cast line in fields by headers
		{
			separator => ","
			autodetect_column_names => true

			# if no headers, you must declare columns
			#columns => ["column1", "column2", "column3"]
		}
		
		if [foo] in ["hello", "world", "foo"] {
		    mutate { add_tag => "field in list" }
	    }
		
		mutate 
		{ 
			split => { "Genre" => "," } 
			split => { "Subgenre" => "," }
			convert => { "Number" => "integer" }
			remove_field => [ "path", "message", "host"]
		}
	}
}
output
{
	if [source] == "500GreatestHits"
	{
		elasticsearch 
		{
			hosts => ["http://elasticsearch:9200/"]
            index => "csv-reading"         # using date in index
			document_id => "CustomId-%{Number}"  # using property in id
		}
		stdout {
			codec => json
		}
	}
}